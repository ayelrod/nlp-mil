{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter Notebook to parse 20_newsgroup data set\n",
    "# Some code taken from https://github.com/gokriznastic/20-newsgroups_text-classification/blob/master/Multinomial%20Naive%20Bayes-%20BOW%20with%20TF.ipynb\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import string\n",
    "import numpy as np\n",
    "from random import seed\n",
    "from random import random\n",
    "from random import randint\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to parse 20 newsgroup data set \n",
    "# input into function is the group we want to focus on - if group == 'sci.space', bags with sci.space will be labeled positive\n",
    "def parse_20newsgroups(group): \n",
    "    # set parameters\n",
    "    num_bags = 100\n",
    "    num_positive_bags = 50\n",
    "    num_instances = 50 \n",
    "    num_features = 200 \n",
    "    positivity_rate = 0.03\n",
    "    \n",
    "    my_path = '20_newsgroups'\n",
    "    \n",
    "    #creating a list of folder names to make valid pathnames later\n",
    "    folders = [f for f in listdir(my_path)]\n",
    "    \n",
    "    \n",
    "    #creating a 2D list to store list of all files in different folders\n",
    "    files = []\n",
    "    for folder_name in folders:\n",
    "        folder_path = join(my_path, folder_name)\n",
    "        files.append([f for f in listdir(folder_path)])\n",
    "    \n",
    "    \n",
    "    #creating a list of pathnames of all the documents\n",
    "    #this would serve to split our dataset into train & test later without any bias\n",
    "    pathname_list = []\n",
    "    for fo in range(len(folders)):\n",
    "        for fi in files[fo]:\n",
    "            pathname_list.append(join(my_path, join(folders[fo], fi)))\n",
    "            \n",
    "    #making an array containing the classes each of the documents belong to\n",
    "    Y = []\n",
    "    for folder_name in folders:\n",
    "        folder_path = join(my_path, folder_name)\n",
    "        num_of_files= len(listdir(folder_path))\n",
    "        for i in range(num_of_files):\n",
    "            Y.append(folder_name)\n",
    "            \n",
    "    #choose documents \n",
    "    # we start with the positive bags\n",
    "    seed(datetime.now())\n",
    "    pathnames = []\n",
    "    # put all positive docs into one list and negative docs in another\n",
    "    positive_docs = [pathname_list[idx] for idx, element in enumerate(pathname_list) if Y[idx] == group]\n",
    "    negative_docs = [pathname_list[idx] for idx, element in enumerate(pathname_list) if Y[idx] != group]\n",
    "    for i in range(num_positive_bags):\n",
    "        path_bag = []\n",
    "        # select one positive bag\n",
    "        random_index = randint(0, len(positive_docs)-1)\n",
    "        current_file = positive_docs[random_index]\n",
    "        positive_docs.pop(random_index) # remove the file we just used, so we don't use it again       \n",
    "        path_bag.append(current_file)\n",
    "        \n",
    "        # select the rest of the bags, with positivity rate specified at the top of the function\n",
    "        for j in range(num_instances - 1):\n",
    "            if(random() < positivity_rate): # insert positive instance\n",
    "                random_index = randint(0, len(positive_docs)-1)\n",
    "                current_file = positive_docs[random_index]\n",
    "                positive_docs.pop(random_index) # remove the file we just used, so we don't use it again         \n",
    "                path_bag.append(current_file)\n",
    "            else: # insert a negative instance\n",
    "                random_index = randint(0, len(negative_docs)-1)\n",
    "                current_file = negative_docs[random_index]\n",
    "                negative_docs.pop(random_index) # remove the file we just used, so we don't use it again       \n",
    "                path_bag.append(current_file)\n",
    "        pathnames.append(path_bag[:])\n",
    "                \n",
    "    # create negative bags\n",
    "    for i in range(num_bags - num_positive_bags):\n",
    "        path_bag = []\n",
    "        for j in range(num_instances):\n",
    "            random_index = randint(0, len(negative_docs)-1)\n",
    "            current_file = negative_docs[random_index]\n",
    "            negative_docs.pop(random_index) # remove the file we just used, so we don't use it again       \n",
    "            path_bag.append(current_file)\n",
    "        pathnames.append(path_bag[:])\n",
    "    \n",
    "    #create vocabulary\n",
    "    vocab = create_vocabulary(num_features, pathnames)\n",
    "    \n",
    "    #create bags\n",
    "    bags = []\n",
    "    bag = []\n",
    "    instance = []\n",
    "    for paths in pathnames: # for each bag of paths in pathnames\n",
    "        bag = []\n",
    "        for path in paths: # for each path in the bag of paths\n",
    "            instance = make_features(vocab, path)\n",
    "            bag.append(instance[:])\n",
    "        bags.append(bag[:])\n",
    "        \n",
    "    # Create Labels\n",
    "    labels = []\n",
    "    for i in range(num_positive_bags):\n",
    "        labels.append(1)\n",
    "    for j in range(num_bags - num_positive_bags):\n",
    "        labels.append(-1)\n",
    "        \n",
    "    return bags, labels, pathnames, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates the list of words we will use\n",
    "def create_vocabulary(num_features, pathnames):\n",
    "    all_words = []\n",
    "    for paths in pathnames:\n",
    "        for path in paths:\n",
    "            f = open(path, \"r\")\n",
    "            text_lines = f.readlines()\n",
    "            text_lines = remove_metadata(text_lines)\n",
    "        \n",
    "            #traverse over all the lines and tokenize each one with the help of helper function: tokenize_sentence\n",
    "            for line in text_lines:\n",
    "                all_words.append(tokenize_sentence(line))\n",
    "    \n",
    "    # turn words into np array for further processing\n",
    "    np_all_words = np.asarray(flatten(all_words))\n",
    "    \n",
    "    # find unique words and their frequency\n",
    "    words, counts = np.unique(np_all_words, return_counts=True)\n",
    "    \n",
    "    # sort words based off their frequency\n",
    "    freq, wrds = (list(i) for i in zip(*(sorted(zip(counts, words), reverse=True))))\n",
    "    \n",
    "    # choose n number of top words\n",
    "    vocab = wrds[0:num_features]\n",
    "    \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make the features for the given pathname\n",
    "def make_features(vocab, pathname):\n",
    "    f = open(pathname, \"r\")\n",
    "    \n",
    "    text_lines = f.readlines()\n",
    "    text_lines = remove_metadata(text_lines)\n",
    "    \n",
    "    doc_words = []\n",
    "    \n",
    "    #traverse over all the lines and tokenize each one with the help of helper function: tokenize_sentence\n",
    "    for line in text_lines:\n",
    "        doc_words.append(tokenize_sentence(line))\n",
    "        \n",
    "    # turn words into np array for further processing\n",
    "    np_doc_words = np.asarray(flatten(doc_words))\n",
    "    \n",
    "    # find unique words and their frequency\n",
    "    words, counts = np.unique(np_doc_words, return_counts=True)    \n",
    "    \n",
    "    # create dictionary words -> counts\n",
    "    dictionary = dict(zip(words,counts))\n",
    "    \n",
    "    # create features\n",
    "    features = []\n",
    "    for i in range(len(vocab)):\n",
    "        features.append(dictionary.get(vocab[i], 0))\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to remove metadata\n",
    "def remove_metadata(lines):\n",
    "    for i in range(len(lines)):\n",
    "        if(lines[i] == '\\n'):\n",
    "            start = i+1\n",
    "            break\n",
    "    new_lines = lines[start:]\n",
    "    return new_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to convert a sentence into list of words\n",
    "def tokenize_sentence(line):\n",
    "    words = line[0:len(line)-1].strip().split(\" \")\n",
    "    words = preprocess(words)\n",
    "    words = remove_stopwords(words)\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to preprocess the words list to remove punctuations\n",
    "\n",
    "def preprocess(words):\n",
    "    #we'll make use of python's translate function,that maps one set of characters to another\n",
    "    #we create an empty mapping table, the third argument allows us to list all of the characters \n",
    "    #to remove during the translation process\n",
    "    \n",
    "    #first we will try to filter out some  unnecessary data like tabs\n",
    "    table = str.maketrans('', '', '\\t')\n",
    "    words = [word.translate(table) for word in words]\n",
    "    \n",
    "    punctuations = (string.punctuation).replace(\"'\", \"\") \n",
    "    # the character: ' appears in a lot of stopwords and changes meaning of words if removed\n",
    "    #hence it is removed from the list of symbols that are to be discarded from the documents\n",
    "    trans_table = str.maketrans('', '', punctuations)\n",
    "    stripped_words = [word.translate(trans_table) for word in words]\n",
    "    \n",
    "    #some white spaces may be added to the list of words, due to the translate function & nature of our documents\n",
    "    #we remove them below\n",
    "    words = [str for str in stripped_words if str]\n",
    "    \n",
    "    #some words are quoted in the documents & as we have not removed ' to maintain the integrity of some stopwords\n",
    "    #we try to unquote such words below\n",
    "    p_words = []\n",
    "    for word in words:\n",
    "        if (word[0] and word[len(word)-1] == \"'\"):\n",
    "            word = word[1:len(word)-1]\n",
    "        elif(word[0] == \"'\"):\n",
    "            word = word[1:len(word)]\n",
    "        else:\n",
    "            word = word\n",
    "        p_words.append(word)\n",
    "    \n",
    "    words = p_words.copy()\n",
    "        \n",
    "    #we will also remove just-numeric strings as they do not have any significant meaning in text classification\n",
    "    words = [word for word in words if not word.isdigit()]\n",
    "    \n",
    "    #we will also remove single character strings\n",
    "    words = [word for word in words if not len(word) == 1]\n",
    "    \n",
    "    #after removal of so many characters it may happen that some strings have become blank, we remove those\n",
    "    words = [str for str in words if str]\n",
    "    \n",
    "    #we also normalize the cases of our words\n",
    "    words = [word.lower() for word in words]\n",
    "    \n",
    "    #we try to remove words with only 2 characters\n",
    "    words = [word for word in words if len(word) > 2]\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to remove stopwords\n",
    "def remove_stopwords(words):\n",
    "    stopwords = ['a', 'about', 'above', 'after', 'again', 'against', 'all', 'am', 'an', 'and', 'any', 'are', \"aren't\", 'as', 'at',\n",
    "     'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', \n",
    "     'can', \"can't\", 'cannot', 'could', \"couldn't\", 'did', \"didn't\", 'do', 'does', \"doesn't\", 'doing', \"don't\", 'down', 'during',\n",
    "     'each', 'few', 'for', 'from', 'further', \n",
    "     'had', \"hadn't\", 'has', \"hasn't\", 'have', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", \"he's\", 'her', 'here', \"here's\",\n",
    "     'hers', 'herself', 'him', 'himself', 'his', 'how', \"how's\",\n",
    "     'i', \"i'd\", \"i'll\", \"i'm\", \"i've\", 'if', 'in', 'into', 'is', \"isn't\", 'it', \"it's\", 'its', 'itself',\n",
    "     \"let's\", 'me', 'more', 'most', \"mustn't\", 'my', 'myself',\n",
    "     'no', 'nor', 'not', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'ought', 'our', 'ours' 'ourselves', 'out', 'over', 'own',\n",
    "     'same', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', \"shouldn't\", 'so', 'some', 'such', \n",
    "     'than', 'that',\"that's\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', \"there's\", 'these', 'they', \"they'd\", \n",
    "     \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 'very', \n",
    "     'was', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", \"we've\", 'were', \"weren't\", 'what', \"what's\", 'when', \"when's\", 'where',\n",
    "     \"where's\", 'which', 'while', 'who', \"who's\", 'whom', 'why', \"why's\",'will', 'with', \"won't\", 'would', \"wouldn't\", \n",
    "     'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves', \n",
    "     'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten', 'hundred', 'thousand', '1st', '2nd', '3rd',\n",
    "     '4th', '5th', '6th', '7th', '8th', '9th', '10th']\n",
    "    words = [word for word in words if not word in stopwords]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a simple helper function to convert a 2D array to 1D, without using numpy\n",
    "def flatten(list):\n",
    "    new_list = []\n",
    "    for i in list:\n",
    "        for j in i:\n",
    "            new_list.append(j)\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = datetime.now()\n",
    "bags, labels, pathnames, vocab = parse_20newsgroups('alt.atheism')\n",
    "time = datetime.now() - time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# bags: 100\n",
      "# instances: 50\n",
      "# features: 200\n",
      "# labels: 100\n",
      "# of paths per bag: 50\n",
      "time taken: 0:00:15.663946\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(\"# bags: \" + str(len(bags)))\n",
    "print(\"# instances: \" + str(len(bags[0])))\n",
    "print(\"# features: \" + str(len(bags[0][0])))\n",
    "print(\"# labels: \" + str(len(labels)))\n",
    "print(\"# of paths per bag: \" + str(len(pathnames[0]))) # This should be equal to the # of instances\n",
    "print(\"time taken: \" + str(time))\n",
    "\n",
    "print(bags[2][24]) # print out a random instance to see what the instances look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
